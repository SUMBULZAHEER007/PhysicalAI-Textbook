"use strict";(self.webpackChunkphysical_ai_textbook=self.webpackChunkphysical_ai_textbook||[]).push([[557],{1492:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>a,contentTitle:()=>o,default:()=>h,frontMatter:()=>l,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"vision","title":"Robot Vision and Perception","description":"Computer vision for robotics applications","source":"@site/docs/05-vision.md","sourceDirName":".","slug":"/vision","permalink":"/PhysicalAI-Textbook/docs/vision","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":5,"frontMatter":{"title":"Robot Vision and Perception","description":"Computer vision for robotics applications","keywords":["vision","perception","robotics"]},"sidebar":"tutorialSidebar","previous":{"title":"ROS 2 Middleware","permalink":"/PhysicalAI-Textbook/docs/ros2"},"next":{"title":"Capstone Project and System Integration","permalink":"/PhysicalAI-Textbook/docs/capstone"}}');var t=i(4848),r=i(8453);const l={title:"Robot Vision and Perception",description:"Computer vision for robotics applications",keywords:["vision","perception","robotics"]},o="Robot Vision and Perception",a={},c=[{value:"Camera Types",id:"camera-types",level:2},{value:"RGB Cameras",id:"rgb-cameras",level:3},{value:"Depth Cameras (RGB-D)",id:"depth-cameras-rgb-d",level:3},{value:"Thermal Cameras",id:"thermal-cameras",level:3},{value:"Object Detection",id:"object-detection",level:2},{value:"Traditional Approaches",id:"traditional-approaches",level:3},{value:"Deep Learning Approaches",id:"deep-learning-approaches",level:3},{value:"Semantic Segmentation",id:"semantic-segmentation",level:2},{value:"Instance Segmentation",id:"instance-segmentation",level:2},{value:"3D Vision",id:"3d-vision",level:2},{value:"Approaches",id:"approaches",level:3},{value:"Applications",id:"applications",level:3},{value:"Feature Detection",id:"feature-detection",level:2},{value:"Visual Servoing",id:"visual-servoing",level:2},{value:"Integration with ROS 2",id:"integration-with-ros-2",level:2},{value:"Real-time Considerations",id:"real-time-considerations",level:2},{value:"Key Computer Vision Techniques",id:"key-computer-vision-techniques",level:2},{value:"Challenges in Robot Vision",id:"challenges-in-robot-vision",level:2},{value:"Solutions",id:"solutions",level:2}];function d(e){const n={h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"robot-vision-and-perception",children:"Robot Vision and Perception"})}),"\n",(0,t.jsx)(n.p,{children:"Vision is critical for humanoid robots. This module covers camera systems, object detection, and scene understanding."}),"\n",(0,t.jsx)(n.h2,{id:"camera-types",children:"Camera Types"}),"\n",(0,t.jsx)(n.h3,{id:"rgb-cameras",children:"RGB Cameras"}),"\n",(0,t.jsx)(n.p,{children:"Standard color cameras for visual perception:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Resolution: 640x480 to 4K"}),"\n",(0,t.jsx)(n.li,{children:"Frame Rate: 30-120 Hz"}),"\n",(0,t.jsx)(n.li,{children:"Use: Object detection, semantic segmentation, visual tracking"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"depth-cameras-rgb-d",children:"Depth Cameras (RGB-D)"}),"\n",(0,t.jsx)(n.p,{children:"Cameras providing both color and depth:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Examples: Intel RealSense, Microsoft Kinect"}),"\n",(0,t.jsx)(n.li,{children:"Depth Range: 0.1 to 10 meters typical"}),"\n",(0,t.jsx)(n.li,{children:"Use: 3D reconstruction, grasping, obstacle avoidance"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"thermal-cameras",children:"Thermal Cameras"}),"\n",(0,t.jsx)(n.p,{children:"Cameras detecting infrared radiation:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Temperature Range: 10-60\xb0C typical"}),"\n",(0,t.jsx)(n.li,{children:"Use: People detection, heat abnormality detection"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"object-detection",children:"Object Detection"}),"\n",(0,t.jsx)(n.p,{children:"Object detection identifies and locates objects in images:"}),"\n",(0,t.jsx)(n.h3,{id:"traditional-approaches",children:"Traditional Approaches"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Haar Cascades: Fast but limited accuracy"}),"\n",(0,t.jsx)(n.li,{children:"HOG + SVM: Good speed/accuracy balance"}),"\n",(0,t.jsx)(n.li,{children:"Sliding Window: Exhaustive search (slow)"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"deep-learning-approaches",children:"Deep Learning Approaches"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"YOLO: Real-time detection (fast)"}),"\n",(0,t.jsx)(n.li,{children:"Faster R-CNN: High accuracy detection"}),"\n",(0,t.jsx)(n.li,{children:"SSD: Balance of speed and accuracy"}),"\n",(0,t.jsx)(n.li,{children:"EfficientNet: Efficient backbone network"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"semantic-segmentation",children:"Semantic Segmentation"}),"\n",(0,t.jsx)(n.p,{children:"Semantic segmentation assigns class labels to every pixel:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Scene Understanding: What is present in the scene?"}),"\n",(0,t.jsx)(n.li,{children:"Navigation: Where can the robot walk?"}),"\n",(0,t.jsx)(n.li,{children:"Manipulation: Where are the graspable objects?"}),"\n",(0,t.jsx)(n.li,{children:"Safety: Identify hazards and obstacles"}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"Common Models:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"DeepLabV3: High-quality segmentation"}),"\n",(0,t.jsx)(n.li,{children:"U-Net: Effective for specialized imaging"}),"\n",(0,t.jsx)(n.li,{children:"SegFormer: Efficient segmentation"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"instance-segmentation",children:"Instance Segmentation"}),"\n",(0,t.jsx)(n.p,{children:"Combines object detection with pixel-level segmentation:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Identifies individual object instances"}),"\n",(0,t.jsx)(n.li,{children:"Provides precise object boundaries"}),"\n",(0,t.jsx)(n.li,{children:"Needed for manipulation of multiple objects"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"3d-vision",children:"3D Vision"}),"\n",(0,t.jsx)(n.p,{children:"Converting 2D images to 3D understanding:"}),"\n",(0,t.jsx)(n.h3,{id:"approaches",children:"Approaches"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Stereo Vision: Two cameras compute depth by triangulation"}),"\n",(0,t.jsx)(n.li,{children:"Structure from Motion: Reconstruct 3D from multiple views"}),"\n",(0,t.jsx)(n.li,{children:"Depth Sensors: Direct depth measurement"}),"\n",(0,t.jsx)(n.li,{children:"Multi-view Geometry: Reconstruct from many viewpoints"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"applications",children:"Applications"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Obstacle avoidance"}),"\n",(0,t.jsx)(n.li,{children:"Object manipulation"}),"\n",(0,t.jsx)(n.li,{children:"Scene reconstruction"}),"\n",(0,t.jsx)(n.li,{children:"Navigation mapping"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"feature-detection",children:"Feature Detection"}),"\n",(0,t.jsx)(n.p,{children:"Finding correspondences between images:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"SIFT: Scale-Invariant Feature Transform"}),"\n",(0,t.jsx)(n.li,{children:"SURF: Speeded Up Robust Features"}),"\n",(0,t.jsx)(n.li,{children:"ORB: Oriented FAST and Rotated BRIEF"}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"Uses:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Visual odometry (estimate robot motion)"}),"\n",(0,t.jsx)(n.li,{children:"Loop closure (recognize revisited locations)"}),"\n",(0,t.jsx)(n.li,{children:"Image registration (align multiple images)"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"visual-servoing",children:"Visual Servoing"}),"\n",(0,t.jsx)(n.p,{children:"Using vision feedback to control robot motion:"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Eye-in-Hand:"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Camera on end-effector"}),"\n",(0,t.jsx)(n.li,{children:"Direct end-effector view"}),"\n",(0,t.jsx)(n.li,{children:"Effective for manipulation"}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Eye-to-Hand:"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Fixed camera location"}),"\n",(0,t.jsx)(n.li,{children:"Broader field of view"}),"\n",(0,t.jsx)(n.li,{children:"Better for whole-body control"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"integration-with-ros-2",children:"Integration with ROS 2"}),"\n",(0,t.jsx)(n.p,{children:"Vision nodes communicate via ROS 2:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Camera Driver: Captures images"}),"\n",(0,t.jsx)(n.li,{children:"Object Detector: Identifies objects"}),"\n",(0,t.jsx)(n.li,{children:"Motion Planner: Plans based on detection"}),"\n",(0,t.jsx)(n.li,{children:"Controller: Executes motion"}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"Each component is a separate ROS 2 node with defined interfaces."}),"\n",(0,t.jsx)(n.h2,{id:"real-time-considerations",children:"Real-time Considerations"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Performance Requirements:"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Frame Rate: 30-120 Hz for interactive tasks"}),"\n",(0,t.jsx)(n.li,{children:"Latency: Less than 100 ms for responsive control"}),"\n",(0,t.jsx)(n.li,{children:"GPU: Needed for deep learning at real-time rates"}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Computational Efficiency:"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Embedded systems: Use lightweight models (MobileNet)"}),"\n",(0,t.jsx)(n.li,{children:"Desktop: Full-size models (ResNet, VGG)"}),"\n",(0,t.jsx)(n.li,{children:"Edge devices: Model quantization and pruning"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"key-computer-vision-techniques",children:"Key Computer Vision Techniques"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Image Processing: Filtering, edge detection, morphology"}),"\n",(0,t.jsx)(n.li,{children:"Feature Extraction: Key points, descriptors"}),"\n",(0,t.jsx)(n.li,{children:"Segmentation: Separating objects from background"}),"\n",(0,t.jsx)(n.li,{children:"Tracking: Following objects across frames"}),"\n",(0,t.jsx)(n.li,{children:"3D Reconstruction: Building 3D models from images"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"challenges-in-robot-vision",children:"Challenges in Robot Vision"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Lighting variations: Shadows, reflections, backlighting"}),"\n",(0,t.jsx)(n.li,{children:"Occlusion: Objects blocked by other objects"}),"\n",(0,t.jsx)(n.li,{children:"Motion blur: Fast robot motion"}),"\n",(0,t.jsx)(n.li,{children:"Real-time constraints: Limited computational budget"}),"\n",(0,t.jsx)(n.li,{children:"Variability: Different object appearances"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"solutions",children:"Solutions"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Multi-sensor fusion: Combine camera, depth, lidar"}),"\n",(0,t.jsx)(n.li,{children:"Robust algorithms: Handle occlusion and outliers"}),"\n",(0,t.jsx)(n.li,{children:"Hardware acceleration: GPU for deep learning"}),"\n",(0,t.jsx)(n.li,{children:"Model adaptation: Fine-tune on robot-specific data"}),"\n",(0,t.jsx)(n.li,{children:"Simplification: Don't solve harder problems than needed"}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Next:"})," Bring everything together in the capstone project."]})]})}function h(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>l,x:()=>o});var s=i(6540);const t={},r=s.createContext(t);function l(e){const n=s.useContext(r);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:l(e.components),s.createElement(r.Provider,{value:n},e.children)}}}]);